{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVq8V0VlLvwE"
   },
   "source": [
    "# DEMO de Elaboración y Ejecución de Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hrg1itwCwfCM"
   },
   "outputs": [],
   "source": [
    "# Poned vuestra API KEY de groq aquí:\n",
    "groq_api_key = \"<GROQ_API_KEY>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86GvZgsNMLuT"
   },
   "source": [
    "###   Clase Benchmark + Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCMl0PLmcZHp"
   },
   "outputs": [],
   "source": [
    "!pip install groq tabulate\n",
    "import json, re, requests\n",
    "from groq import Groq\n",
    "from tabulate import tabulate\n",
    "\n",
    "class LLMBenchmark:\n",
    "    def __init__(self, groq_api_key, models = [\"llama-3.2-90b-vision-preview\", \"llama-3.3-70b-versatile\", \"mixtral-8x7b-32768\", \"gemma2-9b-it\", \"llama-3.2-3b-preview\"]):\n",
    "        self.groq_api_key = groq_api_key\n",
    "        self.groq_client = Groq(api_key=groq_api_key)\n",
    "        self.questions = []\n",
    "        self.models = models\n",
    "\n",
    "    def add_question(self, question, options, correct_answer):\n",
    "        # Formateamos las opciones con letras\n",
    "        formatted_options = {chr(65 + i): option for i, option in enumerate(options)}\n",
    "        formatted_options_inverse = {option : chr(65 + i) for i, option in enumerate(options)}\n",
    "        self.questions.append({\n",
    "            \"question\": question,\n",
    "            \"options\": formatted_options,\n",
    "            \"correct_answer\": formatted_options_inverse[correct_answer] if correct_answer in formatted_options_inverse else correct_answer\n",
    "        })\n",
    "\n",
    "    def print_groq_models(self):\n",
    "        url = \"https://api.groq.com/openai/v1/models\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.groq_api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        print(response.json())\n",
    "\n",
    "    def add_model(self, *model_names):\n",
    "        self.models.extend(model_names)\n",
    "\n",
    "    def print_debug(self, message, debug):\n",
    "        if debug:\n",
    "            print(message)\n",
    "\n",
    "    def run_benchmark(self, debug=False):\n",
    "        results = {}\n",
    "        for model_name in self.models:\n",
    "            correct_predictions = 0\n",
    "            for question_data in self.questions:\n",
    "                question = question_data['question']\n",
    "                options = question_data['options']\n",
    "                correct_answer = question_data['correct_answer']\n",
    "\n",
    "                # Construimos el prompt con las opciones\n",
    "                options_text = \"\\n\".join([f\"{label}: {option}\" for label, option in options.items()])\n",
    "                prompt = f\"{question}\\nOpciones:\\n{options_text}\\n\\nPor favor, elige la letra de la opción correcta y responde solo con esa letra y nada más.\"\n",
    "\n",
    "                try:\n",
    "                    chat_completion = self.groq_client.chat.completions.create(\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                        model=model_name,\n",
    "                        max_tokens=50,\n",
    "                        temperature=0,\n",
    "                    )\n",
    "                    model_answer = chat_completion.choices[0].message.content.strip().upper()\n",
    "\n",
    "                    # Validar la respuesta con regex\n",
    "                    if correct_answer.upper() in model_answer:\n",
    "                        correct_predictions += 1\n",
    "                        self.print_debug(f\"✅ Model: {model_name} | Question: {question} | Answer: {model_answer} | Correct: {correct_answer}\", debug)\n",
    "                    else:\n",
    "                        self.print_debug(f\"❌ Model: {model_name} | Question: {question} | Answer: {model_answer} | Correct: {correct_answer}\", debug)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error querying {model_name}: {e}\")\n",
    "\n",
    "            # Calcula la tasa de aciertos\n",
    "            total_questions = len(self.questions)\n",
    "            accuracy = correct_predictions / total_questions if total_questions else 0\n",
    "            results[model_name] = (correct_predictions, total_questions, accuracy)\n",
    "\n",
    "        # Imprimir resultados en una tabla\n",
    "        headers = [\"Modelo\", \"Respuestas correctas\", \"Exactitud\"]\n",
    "        table = [[model, f\"{correct}/{total}\", f\"{accuracy:.2%}\"] for model, (correct, total, accuracy) in results.items()]\n",
    "        print(tabulate(table, headers=headers, tablefmt='grid'))\n",
    "        #return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HASABXbTzeCx"
   },
   "source": [
    "###   Ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gzRPuft0doqj"
   },
   "outputs": [],
   "source": [
    "llm_benchmark_test = LLMBenchmark(groq_api_key)\n",
    "llm_benchmark_test.add_question(\"¿Cuál es la capital de Francia?\", [\"París\", \"Roma\", \"Berlín\"], \"A\")\n",
    "llm_benchmark_test.add_question(\"¿Cuál es el río más largo del mundo?\", [\"Nilo\", \"Amazonas\", \"Danubio\"], \"Nilo\")\n",
    "llm_benchmark_test.add_question(\"¿En qué año se descubrió América?\", [\"1492\", \"1776\", \"1642\"], \"1492\")\n",
    "llm_benchmark_test.add_question(\"¿Cuál es la fórmula química del agua?\", [\"H2O\", \"CO2\", \"NaCl\"], \"A\")\n",
    "llm_benchmark_test.run_benchmark(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tTJQ_RV6lXle"
   },
   "outputs": [],
   "source": [
    "llm_benchmark_test2 = LLMBenchmark(groq_api_key)\n",
    "llm_benchmark_test2.add_question(\"¿Cuál es el elemento químico más abundante en la corteza terrestre?\", [\"Oxígeno\", \"Silicio\", \"Aluminio\"], \"Oxígeno\")\n",
    "llm_benchmark_test2.run_benchmark(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n23Hj86cMUX_"
   },
   "source": [
    "#   CÓDIGO PRÁCTICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "pP_SQnqA7u1j"
   },
   "outputs": [],
   "source": [
    "llm_benchmark = LLMBenchmark(groq_api_key)\n",
    "# A completar"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP7EMtniiGhcQSnt5Y0Lvjb",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
